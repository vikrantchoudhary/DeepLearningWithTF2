{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[  0   0   1   1   0   1\n    0   0   0   0  34  95\n    0   0   0   0  52  70\n    0   0   0   2   1   0\n    1   0   0   0]\n [  0   0   0   1   1   0\n    0  11  59 111 113 182\n  169 226 255 188 175 162\n  105  85  31   0   0   0\n    1   0   0   0]\n [  0   0   1   0   0  27\n   89 127 127 115 101  86\n   81  95  91  88  78  92\n  115 136 139 126  73   1\n    0   0   0   0]\n [  0   0   0   0  49 117\n  113  95  94  97  98 102\n  101  98  91  97 104  97\n  101  92  95 111 128  82\n    1   0   0   0]\n [  0   0   0  18 118 102\n   92  92  92  89  94  92\n   86  85  88  94  92  92\n   95  99  95  98  89 126\n   24   0   0   0]\n [  0   0   0  59 127 102\n   95  94  97  91  86  91\n   92  88  86  89  91  99\n  102 101  98  94  99 121\n   57   0   0   0]\n [  0   0   0  95 118 107\n   98  89  84  86  86  86\n   89  89  85  85  92  92\n   89  89  88  97 107 111\n   97   0   0   0]\n [  0   0   0 111 126 123\n  111 102 102  94  91  88\n   89  91  86  86  95  97\n   91  98 104 102 111 102\n  111   0   0   0]\n [  0   0   0 108 107 117\n  146 169 111 105  91  91\n   88  84  88  91  92  94\n  105  97 136 162 104  97\n  114   0   0   0]\n [  0   0   1 118 104 114\n  169 130  85  86  82  85\n   85  85  86  88  88  92\n   92  94  95 155 104 104\n  123   5   0   0]\n [  0   0   4 126  97 120\n   92  63 104  82  86  86\n   84  81  82  82  82  89\n   84  99  65  89 130  92\n  120  27   0   0]\n [  0   0  10 123  95 121\n   66  52 117  78  86  84\n   76  86  88  84  84  85\n   81 113  55  47 149  94\n  124  37   0   0]\n [  0   0  14 121  98 136\n   70   7 140  79  88  92\n   81  97  85  85  91  85\n   79 130  27  24 160  98\n  127  43   0   0]\n [  0   0  20 115  91 149\n   46   0 130  88  88 105\n   89  89  85  94  99  92\n   82 123   8  15 160 111\n  121  43   0   0]\n [  0   0  31 118  89 140\n   13   0 113 105  97  91\n   94  88  94  92  97  95\n   89 128   4   0 159 118\n  121  39   0   0]\n [  0   0  42 120  86 133\n    4   0 110 113 101  92\n   89  91  89  97  94  97\n   89 131   0   0 155 117\n  126  55   0   0]\n [  0   0  55 104  86 117\n    1   0 127 111  92  97\n   94  84  98  92  95 101\n   92 130   2   0 134 104\n  114  68   0   0]\n [  0   0  70 104  97 105\n    0   0 130 105  95  97\n   95  89  97  95  92  99\n   98 124  11   0 113 101\n  108  66   0   0]\n [  0   0  76 110  99 115\n    0   1 131 104  98  95\n   98  94  95  98  91  97\n  102 120  11   0  91 118\n  114  68   0   0]\n [  0   0  65 115  99 114\n    0  15 139 102  97  92\n   95  95  94  98  94  97\n  102 124  14   0  79 126\n  118  63   0   0]\n [  0   0  49 118 105  97\n    0  31 139  98 102 102\n  101  98 101 101 101  95\n   94 131  39   0  82 118\n  118  52   0   0]\n [  0   0  43 113 108  91\n    0  63 137  89 101 102\n  107 105 107 107 104 102\n   91 120  55   0  82 124\n  118  47   0   0]\n [  0   0  43 118 114  89\n    0  97 121  95 102 104\n  107 104 104 104  97 104\n   95 111  68   0  79 123\n  115  44   0   0]\n [  0   0  42 120 118  69\n    0 114 111 104 104 105\n  108 108 108 108 102 105\n  104 123  92   0  75 130\n  124  46   0   0]\n [  0   0  44 120 117  63\n    0 149 124 120 120 121\n  124 121 118 118 117 120\n  115 123 117   0  69 117\n  123  49   0   0]\n [  0   0  50 117 117  69\n    1 149 136 131 134 134\n  136 131 130 131 131 137\n  133 143 147   0  75 124\n  117  50   0   0]\n [  0   0  65 156 150  73\n    0   1   0   1   2   2\n    2   2   2   2   2   1\n    2   4   8   0  81 134\n  131  63   0   0]\n [  0   0  14  50  33  10\n    0   0   0   0   0   0\n    0   0   0   0   0   0\n    0   0   0   0  20  55\n   59  17   0   0]]\n"
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "#https://github.com/zalandoresearch/fashion-mnist\n",
    "fashion_mnist_data = keras.datasets.fashion_mnist\n",
    "\n",
    "(train_images,train_label),(test_images,test_label) = fashion_mnist_data.load_data()\n",
    "\n",
    "print(train_images[37])\n",
    "#plt.imshow(train_images[37])\n",
    "#print(test_images.shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[0.         0.\n  0.00392157 0.00392157\n  0.         0.00392157\n  0.         0.\n  0.         0.\n  0.13333333 0.37254902\n  0.         0.\n  0.         0.\n  0.20392157 0.2745098\n  0.         0.\n  0.         0.00784314\n  0.00392157 0.\n  0.00392157 0.\n  0.         0.        ]\n [0.         0.\n  0.         0.00392157\n  0.00392157 0.\n  0.         0.04313725\n  0.23137255 0.43529412\n  0.44313725 0.71372549\n  0.6627451  0.88627451\n  1.         0.7372549\n  0.68627451 0.63529412\n  0.41176471 0.33333333\n  0.12156863 0.\n  0.         0.\n  0.00392157 0.\n  0.         0.        ]\n [0.         0.\n  0.00392157 0.\n  0.         0.10588235\n  0.34901961 0.49803922\n  0.49803922 0.45098039\n  0.39607843 0.3372549\n  0.31764706 0.37254902\n  0.35686275 0.34509804\n  0.30588235 0.36078431\n  0.45098039 0.53333333\n  0.54509804 0.49411765\n  0.28627451 0.00392157\n  0.         0.\n  0.         0.        ]\n [0.         0.\n  0.         0.\n  0.19215686 0.45882353\n  0.44313725 0.37254902\n  0.36862745 0.38039216\n  0.38431373 0.4\n  0.39607843 0.38431373\n  0.35686275 0.38039216\n  0.40784314 0.38039216\n  0.39607843 0.36078431\n  0.37254902 0.43529412\n  0.50196078 0.32156863\n  0.00392157 0.\n  0.         0.        ]\n [0.         0.\n  0.         0.07058824\n  0.4627451  0.4\n  0.36078431 0.36078431\n  0.36078431 0.34901961\n  0.36862745 0.36078431\n  0.3372549  0.33333333\n  0.34509804 0.36862745\n  0.36078431 0.36078431\n  0.37254902 0.38823529\n  0.37254902 0.38431373\n  0.34901961 0.49411765\n  0.09411765 0.\n  0.         0.        ]\n [0.         0.\n  0.         0.23137255\n  0.49803922 0.4\n  0.37254902 0.36862745\n  0.38039216 0.35686275\n  0.3372549  0.35686275\n  0.36078431 0.34509804\n  0.3372549  0.34901961\n  0.35686275 0.38823529\n  0.4        0.39607843\n  0.38431373 0.36862745\n  0.38823529 0.4745098\n  0.22352941 0.\n  0.         0.        ]\n [0.         0.\n  0.         0.37254902\n  0.4627451  0.41960784\n  0.38431373 0.34901961\n  0.32941176 0.3372549\n  0.3372549  0.3372549\n  0.34901961 0.34901961\n  0.33333333 0.33333333\n  0.36078431 0.36078431\n  0.34901961 0.34901961\n  0.34509804 0.38039216\n  0.41960784 0.43529412\n  0.38039216 0.\n  0.         0.        ]\n [0.         0.\n  0.         0.43529412\n  0.49411765 0.48235294\n  0.43529412 0.4\n  0.4        0.36862745\n  0.35686275 0.34509804\n  0.34901961 0.35686275\n  0.3372549  0.3372549\n  0.37254902 0.38039216\n  0.35686275 0.38431373\n  0.40784314 0.4\n  0.43529412 0.4\n  0.43529412 0.\n  0.         0.        ]\n [0.         0.\n  0.         0.42352941\n  0.41960784 0.45882353\n  0.57254902 0.6627451\n  0.43529412 0.41176471\n  0.35686275 0.35686275\n  0.34509804 0.32941176\n  0.34509804 0.35686275\n  0.36078431 0.36862745\n  0.41176471 0.38039216\n  0.53333333 0.63529412\n  0.40784314 0.38039216\n  0.44705882 0.\n  0.         0.        ]\n [0.         0.\n  0.00392157 0.4627451\n  0.40784314 0.44705882\n  0.6627451  0.50980392\n  0.33333333 0.3372549\n  0.32156863 0.33333333\n  0.33333333 0.33333333\n  0.3372549  0.34509804\n  0.34509804 0.36078431\n  0.36078431 0.36862745\n  0.37254902 0.60784314\n  0.40784314 0.40784314\n  0.48235294 0.01960784\n  0.         0.        ]\n [0.         0.\n  0.01568627 0.49411765\n  0.38039216 0.47058824\n  0.36078431 0.24705882\n  0.40784314 0.32156863\n  0.3372549  0.3372549\n  0.32941176 0.31764706\n  0.32156863 0.32156863\n  0.32156863 0.34901961\n  0.32941176 0.38823529\n  0.25490196 0.34901961\n  0.50980392 0.36078431\n  0.47058824 0.10588235\n  0.         0.        ]\n [0.         0.\n  0.03921569 0.48235294\n  0.37254902 0.4745098\n  0.25882353 0.20392157\n  0.45882353 0.30588235\n  0.3372549  0.32941176\n  0.29803922 0.3372549\n  0.34509804 0.32941176\n  0.32941176 0.33333333\n  0.31764706 0.44313725\n  0.21568627 0.18431373\n  0.58431373 0.36862745\n  0.48627451 0.14509804\n  0.         0.        ]\n [0.         0.\n  0.05490196 0.4745098\n  0.38431373 0.53333333\n  0.2745098  0.02745098\n  0.54901961 0.30980392\n  0.34509804 0.36078431\n  0.31764706 0.38039216\n  0.33333333 0.33333333\n  0.35686275 0.33333333\n  0.30980392 0.50980392\n  0.10588235 0.09411765\n  0.62745098 0.38431373\n  0.49803922 0.16862745\n  0.         0.        ]\n [0.         0.\n  0.07843137 0.45098039\n  0.35686275 0.58431373\n  0.18039216 0.\n  0.50980392 0.34509804\n  0.34509804 0.41176471\n  0.34901961 0.34901961\n  0.33333333 0.36862745\n  0.38823529 0.36078431\n  0.32156863 0.48235294\n  0.03137255 0.05882353\n  0.62745098 0.43529412\n  0.4745098  0.16862745\n  0.         0.        ]\n [0.         0.\n  0.12156863 0.4627451\n  0.34901961 0.54901961\n  0.05098039 0.\n  0.44313725 0.41176471\n  0.38039216 0.35686275\n  0.36862745 0.34509804\n  0.36862745 0.36078431\n  0.38039216 0.37254902\n  0.34901961 0.50196078\n  0.01568627 0.\n  0.62352941 0.4627451\n  0.4745098  0.15294118\n  0.         0.        ]\n [0.         0.\n  0.16470588 0.47058824\n  0.3372549  0.52156863\n  0.01568627 0.\n  0.43137255 0.44313725\n  0.39607843 0.36078431\n  0.34901961 0.35686275\n  0.34901961 0.38039216\n  0.36862745 0.38039216\n  0.34901961 0.51372549\n  0.         0.\n  0.60784314 0.45882353\n  0.49411765 0.21568627\n  0.         0.        ]\n [0.         0.\n  0.21568627 0.40784314\n  0.3372549  0.45882353\n  0.00392157 0.\n  0.49803922 0.43529412\n  0.36078431 0.38039216\n  0.36862745 0.32941176\n  0.38431373 0.36078431\n  0.37254902 0.39607843\n  0.36078431 0.50980392\n  0.00784314 0.\n  0.5254902  0.40784314\n  0.44705882 0.26666667\n  0.         0.        ]\n [0.         0.\n  0.2745098  0.40784314\n  0.38039216 0.41176471\n  0.         0.\n  0.50980392 0.41176471\n  0.37254902 0.38039216\n  0.37254902 0.34901961\n  0.38039216 0.37254902\n  0.36078431 0.38823529\n  0.38431373 0.48627451\n  0.04313725 0.\n  0.44313725 0.39607843\n  0.42352941 0.25882353\n  0.         0.        ]\n [0.         0.\n  0.29803922 0.43137255\n  0.38823529 0.45098039\n  0.         0.00392157\n  0.51372549 0.40784314\n  0.38431373 0.37254902\n  0.38431373 0.36862745\n  0.37254902 0.38431373\n  0.35686275 0.38039216\n  0.4        0.47058824\n  0.04313725 0.\n  0.35686275 0.4627451\n  0.44705882 0.26666667\n  0.         0.        ]\n [0.         0.\n  0.25490196 0.45098039\n  0.38823529 0.44705882\n  0.         0.05882353\n  0.54509804 0.4\n  0.38039216 0.36078431\n  0.37254902 0.37254902\n  0.36862745 0.38431373\n  0.36862745 0.38039216\n  0.4        0.48627451\n  0.05490196 0.\n  0.30980392 0.49411765\n  0.4627451  0.24705882\n  0.         0.        ]\n [0.         0.\n  0.19215686 0.4627451\n  0.41176471 0.38039216\n  0.         0.12156863\n  0.54509804 0.38431373\n  0.4        0.4\n  0.39607843 0.38431373\n  0.39607843 0.39607843\n  0.39607843 0.37254902\n  0.36862745 0.51372549\n  0.15294118 0.\n  0.32156863 0.4627451\n  0.4627451  0.20392157\n  0.         0.        ]\n [0.         0.\n  0.16862745 0.44313725\n  0.42352941 0.35686275\n  0.         0.24705882\n  0.5372549  0.34901961\n  0.39607843 0.4\n  0.41960784 0.41176471\n  0.41960784 0.41960784\n  0.40784314 0.4\n  0.35686275 0.47058824\n  0.21568627 0.\n  0.32156863 0.48627451\n  0.4627451  0.18431373\n  0.         0.        ]\n [0.         0.\n  0.16862745 0.4627451\n  0.44705882 0.34901961\n  0.         0.38039216\n  0.4745098  0.37254902\n  0.4        0.40784314\n  0.41960784 0.40784314\n  0.40784314 0.40784314\n  0.38039216 0.40784314\n  0.37254902 0.43529412\n  0.26666667 0.\n  0.30980392 0.48235294\n  0.45098039 0.17254902\n  0.         0.        ]\n [0.         0.\n  0.16470588 0.47058824\n  0.4627451  0.27058824\n  0.         0.44705882\n  0.43529412 0.40784314\n  0.40784314 0.41176471\n  0.42352941 0.42352941\n  0.42352941 0.42352941\n  0.4        0.41176471\n  0.40784314 0.48235294\n  0.36078431 0.\n  0.29411765 0.50980392\n  0.48627451 0.18039216\n  0.         0.        ]\n [0.         0.\n  0.17254902 0.47058824\n  0.45882353 0.24705882\n  0.         0.58431373\n  0.48627451 0.47058824\n  0.47058824 0.4745098\n  0.48627451 0.4745098\n  0.4627451  0.4627451\n  0.45882353 0.47058824\n  0.45098039 0.48235294\n  0.45882353 0.\n  0.27058824 0.45882353\n  0.48235294 0.19215686\n  0.         0.        ]\n [0.         0.\n  0.19607843 0.45882353\n  0.45882353 0.27058824\n  0.00392157 0.58431373\n  0.53333333 0.51372549\n  0.5254902  0.5254902\n  0.53333333 0.51372549\n  0.50980392 0.51372549\n  0.51372549 0.5372549\n  0.52156863 0.56078431\n  0.57647059 0.\n  0.29411765 0.48627451\n  0.45882353 0.19607843\n  0.         0.        ]\n [0.         0.\n  0.25490196 0.61176471\n  0.58823529 0.28627451\n  0.         0.00392157\n  0.         0.00392157\n  0.00784314 0.00784314\n  0.00784314 0.00784314\n  0.00784314 0.00784314\n  0.00784314 0.00392157\n  0.00784314 0.01568627\n  0.03137255 0.\n  0.31764706 0.5254902\n  0.51372549 0.24705882\n  0.         0.        ]\n [0.         0.\n  0.05490196 0.19607843\n  0.12941176 0.03921569\n  0.         0.\n  0.         0.\n  0.         0.\n  0.         0.\n  0.         0.\n  0.         0.\n  0.         0.\n  0.         0.\n  0.07843137 0.21568627\n  0.23137255 0.06666667\n  0.         0.        ]]\n"
    }
   ],
   "source": [
    "#normalizing the training_images and test_images\n",
    "(train_images,test_images) = (train_images/255.0 , test_images/255.0 )\n",
    "print(train_images[37]) #this will make all images data from 0-255 to 0-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 60000 samples\nEpoch 1/15\n60000/60000 [==============================] - 2s 34us/sample - loss: 0.4999\nEpoch 2/15\n60000/60000 [==============================] - 2s 30us/sample - loss: 0.3716\nEpoch 3/15\n60000/60000 [==============================] - 2s 29us/sample - loss: 0.3338\nEpoch 4/15\n60000/60000 [==============================] - 2s 29us/sample - loss: 0.3106\nEpoch 5/15\n60000/60000 [==============================] - 2s 30us/sample - loss: 0.2968\nEpoch 6/15\n60000/60000 [==============================] - 2s 29us/sample - loss: 0.2793\nEpoch 7/15\n60000/60000 [==============================] - 2s 30us/sample - loss: 0.2682\nEpoch 8/15\n60000/60000 [==============================] - 2s 33us/sample - loss: 0.2584\nEpoch 9/15\n60000/60000 [==============================] - 2s 33us/sample - loss: 0.2479\nEpoch 10/15\n60000/60000 [==============================] - 2s 31us/sample - loss: 0.2390\nEpoch 11/15\n60000/60000 [==============================] - 2s 34us/sample - loss: 0.2309\nEpoch 12/15\n60000/60000 [==============================] - 2s 31us/sample - loss: 0.2243\nEpoch 13/15\n60000/60000 [==============================] - 2s 31us/sample - loss: 0.2167\nEpoch 14/15\n60000/60000 [==============================] - 2s 33us/sample - loss: 0.2106\nEpoch 15/15\n60000/60000 [==============================] - 2s 34us/sample - loss: 0.2047\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x14fcbbe50>"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "#model \n",
    "callback = myCallback()\n",
    "model = keras.Sequential([keras.layers.Flatten(),\n",
    "keras.layers.Dense(128 , activation=tf.nn.relu),\n",
    "keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "# flattern , just take images 28*28 to one dimensional set\n",
    "# RELU: postive regression if x>0 and =0 if x < 0\n",
    "#softmax : will use in the output .. it effectevly takes biggest value.\n",
    "\n",
    "model.compile(optimizer= 'adam',\n",
    "    loss='sparse_categorical_crossentropy')\n",
    "model.fit(train_images,train_label,epochs=15, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "10000/10000 [==============================] - 0s 24us/sample - loss: 0.3519\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.35191682986021044"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "#evaluate & test the accuracy\n",
    "model.evaluate(test_images,test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop training with callback\n",
    "class myCallback (tf.keras.callbacks.Callback) :\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('loss') < 0.2) :\n",
    "            print (\"\\n loss reached at acceptable limit\")\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}