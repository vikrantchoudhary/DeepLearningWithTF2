{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "horses data = 500 , humans data = 527  \nValidation horses data = 128 , humans data = 128  \n"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#data downloaded from https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\n",
    "horses_datadir = os.path.join('../data/horse-or-human/horses')\n",
    "humans_datadir = os.path.join('../data/horse-or-human/humans')\n",
    "validation_horses_datadir = os.path.join('../data/validation-horse-or-human/horses')\n",
    "validation_humans_datadir = os.path.join('../data/validation-horse-or-human/humans')\n",
    "\n",
    "horses_names = os.listdir(horses_datadir)\n",
    "humans_names = os.listdir(humans_datadir)\n",
    "validation_horses_names = os.listdir(validation_horses_datadir)\n",
    "validation_humans_names = os.listdir(validation_humans_datadir)\n",
    "\n",
    "#print (validation_humans_names[:10])\n",
    "#print(humans_names[:10])\n",
    "print (\"horses data = {0} , humans data = {1}  \" .format(len(horses_names),len(humans_names)))\n",
    "print (\"Validation horses data = {0} , humans data = {1}  \" .format(len(validation_horses_names),len(validation_humans_names)))\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Found 1031 images belonging to 3 classes.\nFound 256 images belonging to 2 classes.\n"
    }
   ],
   "source": [
    "#preprossing for both validation and training\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1/255)\n",
    "validation_datagen = ImageDataGenerator(rescale=1/255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    '../data/horse-or-human/',\n",
    "    target_size = (300,300),\n",
    "    batch_size = 128,\n",
    "    class_mode = 'binary'\n",
    ")\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    '../data/validation-horse-or-human/',\n",
    "    target_size = (300,300),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_4 (Conv2D)            (None, 298, 298, 16)      448       \n_________________________________________________________________\nmax_pooling2d_4 (MaxPooling2 (None, 149, 149, 16)      0         \n_________________________________________________________________\nconv2d_5 (Conv2D)            (None, 147, 147, 32)      4640      \n_________________________________________________________________\nmax_pooling2d_5 (MaxPooling2 (None, 73, 73, 32)        0         \n_________________________________________________________________\nconv2d_6 (Conv2D)            (None, 71, 71, 64)        18496     \n_________________________________________________________________\nmax_pooling2d_6 (MaxPooling2 (None, 35, 35, 64)        0         \n_________________________________________________________________\nconv2d_7 (Conv2D)            (None, 33, 33, 64)        36928     \n_________________________________________________________________\nmax_pooling2d_7 (MaxPooling2 (None, 16, 16, 64)        0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 16384)             0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 512)               8389120   \n_________________________________________________________________\ndense_3 (Dense)              (None, 1)                 513       \n=================================================================\nTotal params: 8,450,145\nTrainable params: 8,450,145\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "#model \n",
    "import tensorflow as tf \n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(16,(3,3),activation=tf.nn.relu,input_shape=(300,300,3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(32,(3,3),activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(64,(3,3),activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(64,(3,3),activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512,activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(1,activation=tf.nn.sigmoid)\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:sample_weight modes were coerced from\n  ...\n    to  \n  ['...']\nWARNING:tensorflow:sample_weight modes were coerced from\n  ...\n    to  \n  ['...']\nTrain for 8 steps, validate for 8 steps\nEpoch 1/15\n8/8 [==============================] - 26s 3s/step - loss: 1.3799 - accuracy: 0.5338 - val_loss: 0.6870 - val_accuracy: 0.5000\nEpoch 2/15\n8/8 [==============================] - 25s 3s/step - loss: 0.9657 - accuracy: 0.5581 - val_loss: 1.3044 - val_accuracy: 0.5000\nEpoch 3/15\n8/8 [==============================] - 26s 3s/step - loss: 0.7476 - accuracy: 0.6224 - val_loss: 0.5409 - val_accuracy: 0.6719\nEpoch 4/15\n8/8 [==============================] - 26s 3s/step - loss: 0.5428 - accuracy: 0.7564 - val_loss: 0.5942 - val_accuracy: 0.8438\nEpoch 5/15\n8/8 [==============================] - 28s 3s/step - loss: 0.3229 - accuracy: 0.8662 - val_loss: 1.4613 - val_accuracy: 0.7148\nEpoch 6/15\n8/8 [==============================] - 25s 3s/step - loss: 0.3725 - accuracy: 0.8870 - val_loss: 1.0223 - val_accuracy: 0.8555\nEpoch 7/15\n8/8 [==============================] - 25s 3s/step - loss: 0.1997 - accuracy: 0.9302 - val_loss: 0.7393 - val_accuracy: 0.8359\nEpoch 8/15\n8/8 [==============================] - 25s 3s/step - loss: 0.1942 - accuracy: 0.9291 - val_loss: 0.3974 - val_accuracy: 0.9102\nEpoch 9/15\n8/8 [==============================] - 27s 3s/step - loss: 0.1558 - accuracy: 0.9336 - val_loss: 0.9799 - val_accuracy: 0.8555\nEpoch 10/15\n8/8 [==============================] - 31s 4s/step - loss: 0.2997 - accuracy: 0.9280 - val_loss: 0.5515 - val_accuracy: 0.8516\nEpoch 11/15\n8/8 [==============================] - 33s 4s/step - loss: 0.0987 - accuracy: 0.9336 - val_loss: 6.9250 - val_accuracy: 0.6562\nEpoch 12/15\n8/8 [==============================] - 32s 4s/step - loss: 0.2659 - accuracy: 0.9003 - val_loss: 0.8211 - val_accuracy: 0.8594\nEpoch 13/15\n8/8 [==============================] - 33s 4s/step - loss: 0.0038 - accuracy: 0.9823 - val_loss: 2.4779 - val_accuracy: 0.7227\nEpoch 14/15\n8/8 [==============================] - 32s 4s/step - loss: 0.8409 - accuracy: 0.8494 - val_loss: 1.4096 - val_accuracy: 0.8281\nEpoch 15/15\n8/8 [==============================] - 33s 4s/step - loss: 0.0801 - accuracy: 0.9413 - val_loss: 1.3394 - val_accuracy: 0.8828\n"
    }
   ],
   "source": [
    "predict_run = model.fit(train_generator,validation_data=validation_generator,validation_steps=8,steps_per_epoch=8,epochs=15,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] 0.94130677\n"
    }
   ],
   "source": [
    "print(predict_run.epoch, predict_run.history['accuracy'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}